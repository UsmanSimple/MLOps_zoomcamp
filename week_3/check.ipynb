{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12898"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.stat('Homework/data/fhv_tripdata_2021-06.parquet').st_size // (1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18:28:44.127 | INFO    | prefect.engine - Created flow run 'convivial-serval' for flow 'main'\n",
      "18:28:44.133 | INFO    | Flow run 'convivial-serval' - Using task runner 'SequentialTaskRunner'\n",
      "18:28:46.981 | INFO    | Flow run 'convivial-serval' - Created task run 'get_paths-6e696e34-0' for task 'get_paths'\n",
      "18:30:18.249 | INFO    | Task run 'get_paths-6e696e34-0' - fhv_tripdata_2021-06 has been successfully downloaded and placed in the file path- Homework/data/fhv_tripdata_2021-06.parquet\n",
      "18:31:18.874 | INFO    | Task run 'get_paths-6e696e34-0' - fhv_tripdata_2021-07 has been successfully downloaded and placed in the file path- Homework/data/fhv_tripdata_2021-07.parquet\n",
      "18:31:21.269 | INFO    | Task run 'get_paths-6e696e34-0' - Finished in state Completed()\n",
      "18:31:22.816 | INFO    | Flow run 'convivial-serval' - Created task run 'read_data-4c7f9de4-0' for task 'read_data'\n",
      "18:33:11.386 | INFO    | Task run 'read_data-4c7f9de4-0' - Finished in state Completed()\n",
      "18:33:20.096 | INFO    | Flow run 'convivial-serval' - Created task run 'prepare_features-4ee39d9f-0' for task 'prepare_features'\n",
      "18:33:37.987 | INFO    | Task run 'prepare_features-4ee39d9f-0' - The mean duration of training is 18.230538791569113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from prefect import task, flow, get_run_logger\n",
    "from prefect.task_runners import SequentialTaskRunner\n",
    "\n",
    "\n",
    "@task\n",
    "def get_paths(date):\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    date_list = sorted([(pd.to_datetime(date)- relativedelta(months=i)).strftime(\"%Y-%m\") for i in range(1, 3)])\n",
    "\n",
    "    file_paths = []\n",
    "\n",
    "    for i in date_list:\n",
    "\n",
    "        file_name = f\"fhv_tripdata_{i}\"\n",
    "\n",
    "        result= requests.get(f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file_name}.parquet\") \n",
    "\n",
    "        file_path = f\"Homework/data/{file_name}.parquet\"\n",
    "\n",
    "        with open(file_path, 'wb') as file:\n",
    "\n",
    "            file.write(result.content)\n",
    "\n",
    "        logger.info(f'{file_name} has been successfully downloaded and placed in the file path- {file_path}')\n",
    "\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "    return file_paths[0], file_paths[1]\n",
    "\n",
    "@task\n",
    "def read_data(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    return df\n",
    "\n",
    "@task\n",
    "def prepare_features(df, categorical, train=True):\n",
    "    logger = get_run_logger()\n",
    "    df['duration'] = df.dropOff_datetime - df.pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    mean_duration = df.duration.mean()\n",
    "    if train:\n",
    "        logger.info(f\"The mean duration of training is {mean_duration}\")\n",
    "    else:\n",
    "        logger.info(f\"The mean duration of validation is {mean_duration}\")\n",
    "    \n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    return df\n",
    "\n",
    "@task\n",
    "def train_model(df, categorical):\n",
    "\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    train_dicts = df[categorical].to_dict(orient='records')\n",
    "    dv = DictVectorizer()\n",
    "    X_train = dv.fit_transform(train_dicts) \n",
    "    y_train = df.duration.values\n",
    "\n",
    "    logger.info(f\"The shape of X_train is {X_train.shape}\")\n",
    "    logger.info(f\"The DictVectorizer has {len(dv.feature_names_)} features\")\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred = lr.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of training is: {mse}\")\n",
    "    return lr, dv\n",
    "\n",
    "@task\n",
    "def run_model(df, categorical, dv, lr):\n",
    "    logger = get_run_logger()\n",
    "    val_dicts = df[categorical].to_dict(orient='records')\n",
    "    X_val = dv.transform(val_dicts) \n",
    "    y_pred = lr.predict(X_val)\n",
    "    y_val = df.duration.values\n",
    "\n",
    "    mse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "    logger.info(f\"The MSE of validation is: {mse}\")\n",
    "    return\n",
    "\n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(date : str = None):\n",
    "\n",
    "    logger = get_run_logger()\n",
    "\n",
    "    if date is None:\n",
    "        date = datetime.now()\n",
    "    \n",
    "    train_path, val_path = get_paths(date).result()\n",
    "\n",
    "\n",
    "    categorical = ['PUlocationID', 'DOlocationID']\n",
    "\n",
    "    df_train = read_data(train_path)\n",
    "    df_train_processed = prepare_features(df_train, categorical)\n",
    "\n",
    "    df_val = read_data(val_path)\n",
    "    df_val_processed = prepare_features(df_val, categorical, False)\n",
    "\n",
    "    # train the model\n",
    "    lr, dv = train_model(df_train_processed, categorical).result()\n",
    "\n",
    "\n",
    "    with open(f'Homework/models/model-{date}.bin', 'wb') as lin_reg:\n",
    "        pickle.dump(lr, lin_reg )\n",
    "    \n",
    "    dv_path = f'Homework/models/dv-{date}.b'\n",
    "    \n",
    "    with open(dv_path, 'wb') as dict_vect:\n",
    "        pickle.dump(dv, dict_vect)\n",
    "    \n",
    "    dv_size = os.stat(dv_path).st_size\n",
    "\n",
    "    logger.info(f\"The Dictvectorizer size is -: {dv_size} bytes\")\n",
    "        \n",
    "    run_model(df_val_processed, categorical, dv, lr)\n",
    "\n",
    "main(date=\"2021-08-15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import requests\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(\"https://nyc-tlc.s3.amazonaws.com/trip+data/fhv_tripdata_2021-01.parquet\")\n",
    "with open('Homework/data/fhv_tripdata_2021-01.parquet', 'wb') as file_name:\n",
    "    file_name.write(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2022, 6, 13, 10, 42, 41, 292056)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(date):\n",
    "    date_list = sorted([(pandas.to_datetime(date)- relativedelta(months=i)).strftime(\"%Y-%m\") for i in range(1, 3)])\n",
    "    paths = []\n",
    "    for i in date_list:\n",
    "        file_name = f\"fhv_tripdata_{i}\"\n",
    "        result= requests.get(f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file_name}.parquet\") \n",
    "        file_path = f\"Homework/data/{file_name}.parquet\"\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(result.content)\n",
    "        print(f'{file_name} has been successfully downloaded')\n",
    "        paths.append(file_path)\n",
    "    return paths[0], paths[1]\n",
    "\n",
    "def get_date_difference(date : str = None):\n",
    "    if date is None:\n",
    "        date = datetime.now()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-04', '2022-05']\n"
     ]
    }
   ],
   "source": [
    "date = datetime.now()\n",
    "date_list = sorted([(pandas.to_datetime(date)- relativedelta(months=i)).strftime(\"%Y-%m\") for i in range(1, 3)])\n",
    "print(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "950f7c047c791154b8e0edcec1edc80ec392ba6dc6a2ec11f9ffa3af93f70d8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
